
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Zero-Shot MCQA Bench</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.5/dist/css/bootstrap.min.css" rel="stylesheet">
    <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
    <link rel="stylesheet" href="prism.css" data-noprefix />
    <script src="prism.js"></script>
    <style>
    body {
      background-color: #f8f9fa;
      margin-left: 80px; /* Prevent content from being hidden behind the sidebar */
    }
    .sidebar {
      position: fixed;
      left: 0;
      top: 0;
      width: 80px;
      height: 100vh;
      background-color: #343a40;
      display: flex;
      flex-direction: column;
      align-items: center;
      padding-top: 1rem;
    }
    .sidebar a {
      margin-bottom: 1rem;
      display: block;
    }
    .sidebar img {
      max-width: 50px;
      filter: brightness(0.5);
    }
    .sidebar a:hover img {
      filter: brightness(0.8);
    }
  </style>
</head>
<body>
  <!-- Sidebar -->
  <nav class="sidebar">
    <a href="index.html" data-bs-toggle="tooltip" data-bs-placement="right" title="Home">
      <img src="tilde-bench.png" alt="Home Icon" class="img-fluid">
    </a>
    <a href="zero-shot-mcqa-bench.html" data-bs-toggle="tooltip" data-bs-placement="right" title="Zero-shot In-Context Multi-Choice Question-Answering Benchmark">
      <img src="zero-shot-mcqa-bench.png" alt="Zero-shot Icon" class="img-fluid">
    </a>
    <a href="one-shot-empty-bench.html" data-bs-toggle="tooltip" data-bs-placement="right" title="One-shot Sentence-Level Machine Translation Benchmark">
      <img src="one-shot-mt-bench.png" alt="One-shot Icon" class="img-fluid">
    </a>
    <a href="one-shot-empty-robustness-bench.html" data-bs-toggle="tooltip" data-bs-placement="right" title="One-shot Sentence-Level MT Robustness Benchmark">
      <img src="robust-empty-bench.png" alt="One-shot Sentence-Level MT Robustness Benchmark" class="img-fluid">
    </a>
    <a href="tokenizer-bench.html" data-bs-toggle="tooltip" data-bs-placement="right" title="LLM Tokenizer Benchmark">
      <img src="tokenizer-bench.png" alt="Tokenizer Icon" class="img-fluid">
    </a>
    <a href="text-generation-error-analysis-euro-models.html" data-bs-toggle="tooltip" data-bs-placement="right" title="LLM Error Analysis for Languages of the Baltic States">
      <img src="error-analysis.png" alt="Error Analysis Icon" class="img-fluid">
    </a>
  </nav>

  <!-- Main Content -->
  <div class="container py-5">
    <h1>Zero-Shot In-Context Multi-Choice Question-Answering Bench</h1>
    <img src="zero-shot-mcqa-bench.png" width="100" />
    <div><b>Task:</b> In-context question answering (QA) is a the task where a model answers questions based on a provided text passage, using only the given context without relying on external knowledge. In-context question answering is used in all retrieval-augmented generation (RAG) solutions where a generative LLM is tasked to provide an answer to a user's question using documents (or document fragments) retrieved from a semantic database.</div>
    <div><b>Dataset:</b> <a href="https://github.com/facebookresearch/belebele" target="_blank">Belebele dataset</a>. The dataset contains 900 questions for each language. Each question comes along with a text passage (context) and four possible answers. Since the questions are about the text passage and the selection of the best answer relies on understanding, which answer best answers the question based on the given text fragment, this makes the dataset useful for benchmarking in-context QA, albeit a very simple use case where the LLMs task is just to output the correct answer's ID. This benchmark allows to see, which models could be useful for in-context question answering (and RAG solutions) in practice, but as it benchmarks just a simple scenario, make sure you test each model for your language and specifics of your task.</div>
    <div><b>Prompt example:</b></div>
    <div>
    <pre style="width:100%; white-space: pre-wrap; white-space: -pre-wrap; word-wrap: break-word; white-space: -moz-pre-wrap;"><code class="language-json" style="width:800px; white-space: pre-wrap; white-space: -pre-wrap; word-wrap: break-word; white-space: -moz-pre-wrap;">{"messages":
        [{"role": "system", "content": "Answer the question by choosing the answer that can be inferred from the given text. Report only the correct answer's number using the following JSON format: {\"answer_id\":\"\"}."},
        {"role": "user", "content": "Text:\nOne of the most common problems when trying to convert a movie to DVD format is the overscan. Most televisions are made in a way to please the general public. For that reason, everything you see on the TV had the borders cut, top, bottom and sides. This is made to ensure that the image covers the whole screen. That is called overscan. Unfortunately, when you make a DVD, it's borders will most likely be cut too, and if the video had subtitles too close to the bottom, they won't be fully shown.\nQuestion: According to the passage, which of the following problems might one encounter when converting a movie to DVD format?\nPossible answers:\n1) An image that doesn't fill the entire screen\n2) Partially cut subtitles\n3) An image that fills the entire screen\n4) Cut borders\n"}]
}</code></pre>
    </div>
    <div><b>Metrics:</b> Accuracy and below we are also interested to see, whether the LLM's output data in the required format.</div>
    <h2>Accuracy Bar Chart</h2>
    <div>Hover with your mouse over the bars to see top model scores for each language.</div>
    <div id="barChart" style="width:100%;height:600px;"></div>
    <h2>Accuracy Heatmap</h2>
    <div id="heatmap" style="width:100%;height:800px;"></div>
    <h2>Output Format Analysis</h2>
    <div>Here we analyse whether the LLMs are able to follow the instructions to return only the asked JSON object:</div>
    <pre style="width:100%; white-space: pre-wrap; white-space: -pre-wrap; word-wrap: break-word; white-space: -moz-pre-wrap;"><code class="language-json" style="width:800px; white-space: pre-wrap; white-space: -pre-wrap; word-wrap: break-word; white-space: -moz-pre-wrap;">{"answer_id":""}</code></pre>
    <div>We distinguish four levels of obedience:</div>
    <ol>
    <li>Output is the asked JSON object, e.g.:<pre style="width:100%; white-space: pre-wrap; white-space: -pre-wrap; word-wrap: break-word; white-space: -moz-pre-wrap;"><code class="language-json" style="width:800px; white-space: pre-wrap; white-space: -pre-wrap; word-wrap: break-word; white-space: -moz-pre-wrap;">{"answer_id":"1"}</code></pre></li>
    <li>Output contains the asked JSON object, e.g.:<pre style="width:100%; white-space: pre-wrap; white-space: -pre-wrap; word-wrap: break-word; white-space: -moz-pre-wrap;"><code class="language-json" style="width:800px; white-space: pre-wrap; white-space: -pre-wrap; word-wrap: break-word; white-space: -moz-pre-wrap;">```json
{"answer_id":"1"}
```</code></pre></li>
    <li>Output contains a number (it could also be because there is a JSON object, but its structure is wrong), e.g.:<pre style="width:100%; white-space: pre-wrap; white-space: -pre-wrap; word-wrap: break-word; white-space: -moz-pre-wrap;"><code class="language-txt" style="width:800px; white-space: pre-wrap; white-space: -pre-wrap; word-wrap: break-word; white-space: -moz-pre-wrap;">The correct answer is 1</code></pre></li>
    <li>Complete and utter failure (all kinds of hallucination without even a number in it), e.g.:<pre style="width:100%; white-space: pre-wrap; white-space: -pre-wrap; word-wrap: break-word; white-space: -moz-pre-wrap;"><code class="language-txt" style="width:800px; white-space: pre-wrap; white-space: -pre-wrap; word-wrap: break-word; white-space: -moz-pre-wrap;">The sky is blue, isn't it?</code></pre></li>
    </ol>
    <table class='table table-bordered'><thead><tr><th>Model</th><th>Output is the<br />required JSON object</th><th>Output contains the<br />required JSON object</th><th>Output contains<br />a number</th><th>Output is a<br />complete failure</th></tr></thead><tbody><tr><td>BSC-LT-salamandra-7b-instruct</td><td>0.000</td><td>0.000</td><td>0.982</td><td style='color: red;'>0.018</td></tr><tr><td>claude-3-5-haiku-20241022</td><td>0.000</td><td>0.999</td><td>0.001</td><td style='color: red;'>0.000</td></tr><tr><td>claude-3-7-sonnet-20250219</td><td>0.000</td><td>0.999</td><td>0.000</td><td style='color: red;'>0.001</td></tr><tr><td>dolphin3</td><td>0.000</td><td>0.998</td><td>0.002</td><td style='color: red;'>0.000</td></tr><tr><td>gemma2:27b</td><td>0.000</td><td>0.995</td><td>0.005</td><td style='color: red;'>0.001</td></tr><tr><td>gemma2:9b</td><td>0.000</td><td>0.993</td><td>0.006</td><td style='color: red;'>0.000</td></tr><tr><td>gemma3</td><td>0.000</td><td>0.997</td><td>0.003</td><td style='color: red;'>0.000</td></tr><tr><td>gemma3:12b</td><td>0.000</td><td>1.000</td><td>0.000</td><td style='color: red;'>0.000</td></tr><tr><td>gemma3:27b</td><td>0.000</td><td>1.000</td><td>0.000</td><td style='color: red;'>0.000</td></tr><tr><td>gpt-3.5-turbo</td><td>0.000</td><td>1.000</td><td>0.000</td><td style='color: red;'>0.000</td></tr><tr><td>gpt-4o</td><td>0.000</td><td>1.000</td><td>0.000</td><td style='color: red;'>0.000</td></tr><tr><td>gpt-4o-mini</td><td>0.000</td><td>1.000</td><td>0.000</td><td style='color: red;'>0.000</td></tr><tr><td>llama3.1</td><td>0.000</td><td>0.999</td><td>0.001</td><td style='color: red;'>0.000</td></tr><tr><td>llama3.1:70b</td><td>0.000</td><td>0.999</td><td>0.001</td><td style='color: red;'>0.000</td></tr><tr><td>llama3.2</td><td>0.000</td><td>1.000</td><td>0.000</td><td style='color: red;'>0.000</td></tr><tr><td>llama3.3</td><td>0.000</td><td>0.999</td><td>0.001</td><td style='color: red;'>0.000</td></tr><tr><td>mistral-large</td><td>0.000</td><td>0.999</td><td>0.000</td><td style='color: red;'>0.001</td></tr><tr><td>mistral-nemo</td><td>0.000</td><td>0.999</td><td>0.000</td><td style='color: red;'>0.001</td></tr><tr><td>mistral-small</td><td>0.000</td><td>0.998</td><td>0.001</td><td style='color: red;'>0.000</td></tr><tr><td>mistral-small3.1</td><td>0.000</td><td>1.000</td><td>0.000</td><td style='color: red;'>0.000</td></tr><tr><td>nemotron</td><td>0.000</td><td>0.990</td><td>0.010</td><td style='color: red;'>0.000</td></tr><tr><td>olmo2:13b</td><td>0.000</td><td>0.982</td><td>0.018</td><td style='color: red;'>0.000</td></tr><tr><td>openGPT-X-Teuken-7B-instruct-commercial-v0.4</td><td>0.000</td><td>0.538</td><td>0.458</td><td style='color: red;'>0.004</td></tr><tr><td>openGPT-X-Teuken-7B-instruct-research-v0.4</td><td>0.000</td><td>0.675</td><td>0.314</td><td style='color: red;'>0.010</td></tr><tr><td>phi4</td><td>0.000</td><td>0.993</td><td>0.006</td><td style='color: red;'>0.000</td></tr><tr><td>phi4-mini</td><td>0.000</td><td>1.000</td><td>0.000</td><td style='color: red;'>0.000</td></tr><tr><td>qwen2.5:1.5b</td><td>0.000</td><td>0.674</td><td>0.323</td><td style='color: red;'>0.003</td></tr><tr><td>qwen2.5:72b</td><td>0.000</td><td>0.997</td><td>0.002</td><td style='color: red;'>0.001</td></tr><tr><td>utter-project-EuroLLM-1.7B-Instruct</td><td>0.000</td><td>0.000</td><td>1.000</td><td style='color: red;'>0.000</td></tr><tr><td>utter-project-EuroLLM-9B-Instruct</td><td>0.000</td><td>0.044</td><td>0.947</td><td style='color: red;'>0.009</td></tr></tbody></table>
  </div>
    <script>
        // Bar chart data and layout
        var barChart = {"data": [{"x": ["cs", "de", "en", "et", "fi", "fr", "lt", "lv", "pl", "ru", "uk"], "y": [0.72, 0.81, 0.88, 0.516, 0.727, 0.814, 0.589, 0.567, 0.719, 0.806, 0.739], "name": "phi4-mini", "type": "bar"}, {"x": ["cs", "de", "en", "et", "fi", "fr", "lt", "lv", "pl", "ru", "uk"], "y": [0.926, 0.95, 0.962, 0.919, 0.927, 0.944, 0.91, 0.917, 0.927, 0.926, 0.934], "name": "nemotron", "type": "bar"}, {"x": ["cs", "de", "en", "et", "fi", "fr", "lt", "lv", "pl", "ru", "uk"], "y": [0.921, 0.93, 0.949, 0.899, 0.919, 0.931, 0.909, 0.926, 0.906, 0.922, 0.91], "name": "gemma2:27b", "type": "bar"}, {"x": ["cs", "de", "en", "et", "fi", "fr", "lt", "lv", "pl", "ru", "uk"], "y": [0.826, 0.858, 0.913, 0.727, 0.791, 0.88, 0.752, 0.74, 0.812, 0.878, 0.817], "name": "mistral-nemo", "type": "bar"}, {"x": ["cs", "de", "en", "et", "fi", "fr", "lt", "lv", "pl", "ru", "uk"], "y": [0.383, 0.346, 0.434, 0.326, 0.371, 0.36, 0.343, 0.354, 0.388, 0.427, 0.423], "name": "qwen2.5:1.5b", "type": "bar"}, {"x": ["cs", "de", "en", "et", "fi", "fr", "lt", "lv", "pl", "ru", "uk"], "y": [0.931, 0.949, 0.964, 0.89, 0.928, 0.951, 0.897, 0.927, 0.926, 0.939, 0.933], "name": "qwen2.5:72b", "type": "bar"}, {"x": ["cs", "de", "en", "et", "fi", "fr", "lt", "lv", "pl", "ru", "uk"], "y": [0.706, 0.737, 0.771, 0.664, 0.714, 0.703, 0.716, 0.681, 0.684, 0.721, 0.691], "name": "BSC-LT-salamandra-7b-instruct", "type": "bar"}, {"x": ["cs", "de", "en", "et", "fi", "fr", "lt", "lv", "pl", "ru", "uk"], "y": [0.904, 0.933, 0.946, 0.788, 0.889, 0.933, 0.857, 0.836, 0.908, 0.907, 0.902], "name": "phi4", "type": "bar"}, {"x": ["cs", "de", "en", "et", "fi", "fr", "lt", "lv", "pl", "ru", "uk"], "y": [0.911, 0.91, 0.938, 0.869, 0.901, 0.916, 0.874, 0.882, 0.89, 0.907, 0.899], "name": "gemma2:9b", "type": "bar"}, {"x": ["cs", "de", "en", "et", "fi", "fr", "lt", "lv", "pl", "ru", "uk"], "y": [0.44, 0.421, 0.392, 0.412, 0.411, 0.448, 0.467, 0.432, 0.431, 0.428, 0.401], "name": "openGPT-X-Teuken-7B-instruct-commercial-v0.4", "type": "bar"}, {"x": ["cs", "de", "en", "et", "fi", "fr", "lt", "lv", "pl", "ru", "uk"], "y": [0.941, 0.95, 0.966, 0.937, 0.942, 0.956, 0.948, 0.951, 0.938, 0.949, 0.949], "name": "gpt-4o", "type": "bar"}, {"x": ["cs", "de", "en", "et", "fi", "fr", "lt", "lv", "pl", "ru", "uk"], "y": [0.904, 0.911, 0.947, 0.889, 0.908, 0.916, 0.881, 0.876, 0.899, 0.906, 0.902], "name": "gpt-4o-mini", "type": "bar"}, {"x": ["cs", "de", "en", "et", "fi", "fr", "lt", "lv", "pl", "ru", "uk"], "y": [0.926, 0.943, 0.956, 0.897, 0.929, 0.943, 0.908, 0.92, 0.916, 0.926, 0.933], "name": "llama3.3", "type": "bar"}, {"x": ["cs", "de", "en", "et", "fi", "fr", "lt", "lv", "pl", "ru", "uk"], "y": [0.612, 0.761, 0.869, 0.438, 0.614, 0.799, 0.501, 0.477, 0.598, 0.736, 0.564], "name": "olmo2:13b", "type": "bar"}, {"x": ["cs", "de", "en", "et", "fi", "fr", "lt", "lv", "pl", "ru", "uk"], "y": [0.888, 0.901, 0.923, 0.872, 0.892, 0.909, 0.887, 0.882, 0.901, 0.903, 0.898], "name": "gemma3:12b", "type": "bar"}, {"x": ["cs", "de", "en", "et", "fi", "fr", "lt", "lv", "pl", "ru", "uk"], "y": [0.937, 0.953, 0.961, 0.926, 0.94, 0.95, 0.924, 0.944, 0.926, 0.946, 0.944], "name": "claude-3-7-sonnet-20250219", "type": "bar"}, {"x": ["cs", "de", "en", "et", "fi", "fr", "lt", "lv", "pl", "ru", "uk"], "y": [0.689, 0.691, 0.76, 0.58, 0.669, 0.721, 0.64, 0.61, 0.668, 0.686, 0.668], "name": "gemma3", "type": "bar"}, {"x": ["cs", "de", "en", "et", "fi", "fr", "lt", "lv", "pl", "ru", "uk"], "y": [0.259, 0.29, 0.273, 0.272, 0.272, 0.254, 0.273, 0.28, 0.241, 0.256, 0.264], "name": "utter-project-EuroLLM-1.7B-Instruct", "type": "bar"}, {"x": ["cs", "de", "en", "et", "fi", "fr", "lt", "lv", "pl", "ru", "uk"], "y": [0.84, 0.87, 0.91, 0.707, 0.779, 0.868, 0.724, 0.71, 0.809, 0.86, 0.811], "name": "llama3.1", "type": "bar"}, {"x": ["cs", "de", "en", "et", "fi", "fr", "lt", "lv", "pl", "ru", "uk"], "y": [0.913, 0.941, 0.957, 0.867, 0.906, 0.929, 0.861, 0.832, 0.913, 0.919, 0.914], "name": "llama3.1:70b", "type": "bar"}, {"x": ["cs", "de", "en", "et", "fi", "fr", "lt", "lv", "pl", "ru", "uk"], "y": [0.591, 0.629, 0.661, 0.6, 0.593, 0.643, 0.613, 0.617, 0.579, 0.63, 0.629], "name": "utter-project-EuroLLM-9B-Instruct", "type": "bar"}, {"x": ["cs", "de", "en", "et", "fi", "fr", "lt", "lv", "pl", "ru", "uk"], "y": [0.728, 0.8, 0.868, 0.551, 0.624, 0.787, 0.603, 0.572, 0.698, 0.781, 0.734], "name": "dolphin3", "type": "bar"}, {"x": ["cs", "de", "en", "et", "fi", "fr", "lt", "lv", "pl", "ru", "uk"], "y": [0.83, 0.861, 0.888, 0.798, 0.811, 0.854, 0.748, 0.771, 0.81, 0.824, 0.779], "name": "gpt-3.5-turbo", "type": "bar"}, {"x": ["cs", "de", "en", "et", "fi", "fr", "lt", "lv", "pl", "ru", "uk"], "y": [0.918, 0.941, 0.959, 0.908, 0.929, 0.944, 0.889, 0.89, 0.919, 0.928, 0.923], "name": "mistral-large", "type": "bar"}, {"x": ["cs", "de", "en", "et", "fi", "fr", "lt", "lv", "pl", "ru", "uk"], "y": [0.668, 0.724, 0.79, 0.482, 0.599, 0.726, 0.509, 0.48, 0.648, 0.696, 0.611], "name": "llama3.2", "type": "bar"}, {"x": ["cs", "de", "en", "et", "fi", "fr", "lt", "lv", "pl", "ru", "uk"], "y": [0.927, 0.942, 0.958, 0.89, 0.924, 0.942, 0.884, 0.899, 0.911, 0.92, 0.917], "name": "mistral-small3.1", "type": "bar"}, {"x": ["cs", "de", "en", "et", "fi", "fr", "lt", "lv", "pl", "ru", "uk"], "y": [0.459, 0.434, 0.409, 0.434, 0.436, 0.457, 0.457, 0.446, 0.419, 0.432, 0.353], "name": "openGPT-X-Teuken-7B-instruct-research-v0.4", "type": "bar"}, {"x": ["cs", "de", "en", "et", "fi", "fr", "lt", "lv", "pl", "ru", "uk"], "y": [0.917, 0.927, 0.94, 0.901, 0.908, 0.922, 0.899, 0.901, 0.894, 0.913, 0.913], "name": "gemma3:27b", "type": "bar"}, {"x": ["cs", "de", "en", "et", "fi", "fr", "lt", "lv", "pl", "ru", "uk"], "y": [0.922, 0.94, 0.953, 0.888, 0.918, 0.944, 0.866, 0.893, 0.893, 0.933, 0.913], "name": "mistral-small", "type": "bar"}, {"x": ["cs", "de", "en", "et", "fi", "fr", "lt", "lv", "pl", "ru", "uk"], "y": [0.917, 0.921, 0.951, 0.881, 0.919, 0.938, 0.88, 0.908, 0.908, 0.912, 0.922], "name": "claude-3-5-haiku-20241022", "type": "bar"}], "layout": {"title": "Zero-Shot In-Context Multi-Choice Question-Answering (Belebele dataset)", "barmode": "group", "yaxis": {"title": "Accuracy Score"}, "xaxis": {"title": "Language"}}};
        Plotly.newPlot('barChart', barChart.data, barChart.layout);

        // Heatmap data and layout
        var heatmapChart = {"data": [{"x": ["cs", "de", "en", "et", "fi", "fr", "lt", "lv", "pl", "ru", "uk", "Average"], "y": ["gpt-4o", "claude-3-7-sonnet-20250219", "nemotron", "qwen2.5:72b", "llama3.3", "mistral-large", "gemma2:27b", "mistral-small3.1", "mistral-small", "claude-3-5-haiku-20241022", "gemma3:27b", "llama3.1:70b", "gpt-4o-mini", "gemma2:9b", "gemma3:12b", "phi4", "mistral-nemo", "gpt-3.5-turbo", "llama3.1", "phi4-mini", "BSC-LT-salamandra-7b-instruct", "dolphin3", "gemma3", "olmo2:13b", "llama3.2", "utter-project-EuroLLM-9B-Instruct", "openGPT-X-Teuken-7B-instruct-research-v0.4", "openGPT-X-Teuken-7B-instruct-commercial-v0.4", "qwen2.5:1.5b", "utter-project-EuroLLM-1.7B-Instruct"], "z": [[0.9411111111111111, 0.95, 0.9655555555555555, 0.9366666666666666, 0.9422222222222222, 0.9555555555555556, 0.9477777777777778, 0.9511111111111111, 0.9377777777777778, 0.9488888888888889, 0.9488888888888889, 0.9477777777777778], [0.9366666666666666, 0.9533333333333334, 0.9611111111111111, 0.9255555555555556, 0.94, 0.95, 0.9244444444444444, 0.9444444444444444, 0.9255555555555556, 0.9455555555555556, 0.9444444444444444, 0.941010101010101], [0.9255555555555556, 0.95, 0.9622222222222222, 0.9188888888888889, 0.9266666666666666, 0.9444444444444444, 0.91, 0.9166666666666666, 0.9266666666666666, 0.9255555555555556, 0.9344444444444444, 0.9310101010101011], [0.9311111111111111, 0.9488888888888889, 0.9644444444444444, 0.89, 0.9277777777777778, 0.9511111111111111, 0.8966666666666666, 0.9266666666666666, 0.9255555555555556, 0.9388888888888889, 0.9333333333333333, 0.9304040404040405], [0.9255555555555556, 0.9433333333333334, 0.9555555555555556, 0.8966666666666666, 0.9288888888888889, 0.9433333333333334, 0.9077777777777778, 0.92, 0.9155555555555556, 0.9255555555555556, 0.9333333333333333, 0.9268686868686868], [0.9177777777777778, 0.9411111111111111, 0.9588888888888889, 0.9077777777777778, 0.9288888888888889, 0.9444444444444444, 0.8888888888888888, 0.89, 0.9188888888888889, 0.9277777777777778, 0.9233333333333333, 0.9225252525252525], [0.9211111111111111, 0.93, 0.9488888888888889, 0.8988888888888888, 0.9188888888888889, 0.9311111111111111, 0.9088888888888889, 0.9255555555555556, 0.9055555555555556, 0.9222222222222223, 0.91, 0.9201010101010101], [0.9266666666666666, 0.9422222222222222, 0.9577777777777777, 0.89, 0.9244444444444444, 0.9422222222222222, 0.8844444444444445, 0.8988888888888888, 0.9111111111111111, 0.92, 0.9166666666666666, 0.9194949494949495], [0.9222222222222223, 0.94, 0.9533333333333334, 0.8877777777777778, 0.9177777777777778, 0.9444444444444444, 0.8655555555555555, 0.8933333333333333, 0.8933333333333333, 0.9333333333333333, 0.9133333333333333, 0.914949494949495], [0.9166666666666666, 0.9211111111111111, 0.9511111111111111, 0.8811111111111111, 0.9188888888888889, 0.9377777777777778, 0.88, 0.9077777777777778, 0.9077777777777778, 0.9122222222222223, 0.9222222222222223, 0.9142424242424242], [0.9166666666666666, 0.9266666666666666, 0.94, 0.9011111111111111, 0.9077777777777778, 0.9222222222222223, 0.8988888888888888, 0.9011111111111111, 0.8944444444444445, 0.9133333333333333, 0.9133333333333333, 0.9123232323232323], [0.9133333333333333, 0.9411111111111111, 0.9566666666666667, 0.8666666666666667, 0.9055555555555556, 0.9288888888888889, 0.8611111111111112, 0.8322222222222222, 0.9133333333333333, 0.9188888888888889, 0.9144444444444444, 0.9047474747474747], [0.9044444444444445, 0.9111111111111111, 0.9466666666666667, 0.8888888888888888, 0.9077777777777778, 0.9155555555555556, 0.8811111111111111, 0.8755555555555555, 0.8988888888888888, 0.9055555555555556, 0.9022222222222223, 0.9034343434343435], [0.9111111111111111, 0.91, 0.9377777777777778, 0.8688888888888889, 0.9011111111111111, 0.9155555555555556, 0.8744444444444445, 0.8822222222222222, 0.89, 0.9066666666666666, 0.8988888888888888, 0.8996969696969697], [0.8877777777777778, 0.9011111111111111, 0.9233333333333333, 0.8722222222222222, 0.8922222222222222, 0.9088888888888889, 0.8866666666666667, 0.8822222222222222, 0.9011111111111111, 0.9033333333333333, 0.8977777777777778, 0.8960606060606061], [0.9044444444444445, 0.9333333333333333, 0.9455555555555556, 0.7877777777777778, 0.8888888888888888, 0.9333333333333333, 0.8566666666666667, 0.8355555555555556, 0.9077777777777778, 0.9066666666666666, 0.9022222222222223, 0.8911111111111111], [0.8255555555555556, 0.8577777777777778, 0.9133333333333333, 0.7266666666666667, 0.7911111111111111, 0.88, 0.7522222222222222, 0.74, 0.8122222222222222, 0.8777777777777778, 0.8166666666666667, 0.8175757575757576], [0.83, 0.8611111111111112, 0.8877777777777778, 0.7977777777777778, 0.8111111111111111, 0.8544444444444445, 0.7477777777777778, 0.7711111111111111, 0.81, 0.8244444444444444, 0.7788888888888889, 0.8158585858585858], [0.84, 0.87, 0.91, 0.7066666666666667, 0.7788888888888889, 0.8677777777777778, 0.7244444444444444, 0.71, 0.8088888888888889, 0.86, 0.8111111111111111, 0.807979797979798], [0.72, 0.81, 0.88, 0.5155555555555555, 0.7266666666666667, 0.8144444444444444, 0.5888888888888889, 0.5666666666666667, 0.7188888888888889, 0.8055555555555556, 0.7388888888888889, 0.7168686868686869], [0.7055555555555556, 0.7366666666666667, 0.7711111111111111, 0.6644444444444444, 0.7144444444444444, 0.7033333333333334, 0.7155555555555555, 0.6811111111111111, 0.6844444444444444, 0.7211111111111111, 0.6911111111111111, 0.7080808080808081], [0.7277777777777777, 0.8, 0.8677777777777778, 0.5511111111111111, 0.6244444444444445, 0.7866666666666666, 0.6033333333333334, 0.5722222222222222, 0.6977777777777778, 0.7811111111111111, 0.7344444444444445, 0.7042424242424242], [0.6888888888888889, 0.6911111111111111, 0.76, 0.58, 0.6688888888888889, 0.7211111111111111, 0.64, 0.61, 0.6677777777777778, 0.6855555555555556, 0.6677777777777778, 0.6710101010101011], [0.6122222222222222, 0.7611111111111111, 0.8688888888888889, 0.43777777777777777, 0.6144444444444445, 0.7988888888888889, 0.5011111111111111, 0.4766666666666667, 0.5977777777777777, 0.7355555555555555, 0.5644444444444444, 0.6335353535353535], [0.6677777777777778, 0.7244444444444444, 0.79, 0.4822222222222222, 0.5988888888888889, 0.7255555555555555, 0.5088888888888888, 0.48, 0.6477777777777778, 0.6955555555555556, 0.6111111111111112, 0.6302020202020202], [0.5911111111111111, 0.6288888888888889, 0.6611111111111111, 0.6, 0.5933333333333334, 0.6433333333333333, 0.6133333333333333, 0.6166666666666667, 0.5788888888888889, 0.63, 0.6288888888888889, 0.6168686868686869], [0.4588888888888889, 0.43444444444444447, 0.4088888888888889, 0.43444444444444447, 0.43555555555555553, 0.45666666666666667, 0.45666666666666667, 0.44555555555555554, 0.41888888888888887, 0.43222222222222223, 0.35333333333333333, 0.4305050505050505], [0.44, 0.4211111111111111, 0.39222222222222225, 0.4122222222222222, 0.4111111111111111, 0.4477777777777778, 0.4666666666666667, 0.43222222222222223, 0.4311111111111111, 0.42777777777777776, 0.4011111111111111, 0.4257575757575758], [0.38333333333333336, 0.34555555555555556, 0.43444444444444447, 0.32555555555555554, 0.3711111111111111, 0.36, 0.3433333333333333, 0.35444444444444445, 0.3877777777777778, 0.4266666666666667, 0.42333333333333334, 0.37777777777777777], [0.2588888888888889, 0.29, 0.2733333333333333, 0.2722222222222222, 0.2722222222222222, 0.2544444444444444, 0.2733333333333333, 0.28, 0.2411111111111111, 0.25555555555555554, 0.2644444444444444, 0.26686868686868687]], "type": "heatmap", "colorscale": [[0, "#550000"], [0.5, "red"], [0.8, "orange"], [0.9, "green"], [1.0, "#003300"]], "text": [["0.941", "0.950", "0.966", "0.937", "0.942", "0.956", "0.948", "0.951", "0.938", "0.949", "0.949", "0.948"], ["0.937", "0.953", "0.961", "0.926", "0.940", "0.950", "0.924", "0.944", "0.926", "0.946", "0.944", "0.941"], ["0.926", "0.950", "0.962", "0.919", "0.927", "0.944", "0.910", "0.917", "0.927", "0.926", "0.934", "0.931"], ["0.931", "0.949", "0.964", "0.890", "0.928", "0.951", "0.897", "0.927", "0.926", "0.939", "0.933", "0.930"], ["0.926", "0.943", "0.956", "0.897", "0.929", "0.943", "0.908", "0.920", "0.916", "0.926", "0.933", "0.927"], ["0.918", "0.941", "0.959", "0.908", "0.929", "0.944", "0.889", "0.890", "0.919", "0.928", "0.923", "0.923"], ["0.921", "0.930", "0.949", "0.899", "0.919", "0.931", "0.909", "0.926", "0.906", "0.922", "0.910", "0.920"], ["0.927", "0.942", "0.958", "0.890", "0.924", "0.942", "0.884", "0.899", "0.911", "0.920", "0.917", "0.919"], ["0.922", "0.940", "0.953", "0.888", "0.918", "0.944", "0.866", "0.893", "0.893", "0.933", "0.913", "0.915"], ["0.917", "0.921", "0.951", "0.881", "0.919", "0.938", "0.880", "0.908", "0.908", "0.912", "0.922", "0.914"], ["0.917", "0.927", "0.940", "0.901", "0.908", "0.922", "0.899", "0.901", "0.894", "0.913", "0.913", "0.912"], ["0.913", "0.941", "0.957", "0.867", "0.906", "0.929", "0.861", "0.832", "0.913", "0.919", "0.914", "0.905"], ["0.904", "0.911", "0.947", "0.889", "0.908", "0.916", "0.881", "0.876", "0.899", "0.906", "0.902", "0.903"], ["0.911", "0.910", "0.938", "0.869", "0.901", "0.916", "0.874", "0.882", "0.890", "0.907", "0.899", "0.900"], ["0.888", "0.901", "0.923", "0.872", "0.892", "0.909", "0.887", "0.882", "0.901", "0.903", "0.898", "0.896"], ["0.904", "0.933", "0.946", "0.788", "0.889", "0.933", "0.857", "0.836", "0.908", "0.907", "0.902", "0.891"], ["0.826", "0.858", "0.913", "0.727", "0.791", "0.880", "0.752", "0.740", "0.812", "0.878", "0.817", "0.818"], ["0.830", "0.861", "0.888", "0.798", "0.811", "0.854", "0.748", "0.771", "0.810", "0.824", "0.779", "0.816"], ["0.840", "0.870", "0.910", "0.707", "0.779", "0.868", "0.724", "0.710", "0.809", "0.860", "0.811", "0.808"], ["0.720", "0.810", "0.880", "0.516", "0.727", "0.814", "0.589", "0.567", "0.719", "0.806", "0.739", "0.717"], ["0.706", "0.737", "0.771", "0.664", "0.714", "0.703", "0.716", "0.681", "0.684", "0.721", "0.691", "0.708"], ["0.728", "0.800", "0.868", "0.551", "0.624", "0.787", "0.603", "0.572", "0.698", "0.781", "0.734", "0.704"], ["0.689", "0.691", "0.760", "0.580", "0.669", "0.721", "0.640", "0.610", "0.668", "0.686", "0.668", "0.671"], ["0.612", "0.761", "0.869", "0.438", "0.614", "0.799", "0.501", "0.477", "0.598", "0.736", "0.564", "0.634"], ["0.668", "0.724", "0.790", "0.482", "0.599", "0.726", "0.509", "0.480", "0.648", "0.696", "0.611", "0.630"], ["0.591", "0.629", "0.661", "0.600", "0.593", "0.643", "0.613", "0.617", "0.579", "0.630", "0.629", "0.617"], ["0.459", "0.434", "0.409", "0.434", "0.436", "0.457", "0.457", "0.446", "0.419", "0.432", "0.353", "0.431"], ["0.440", "0.421", "0.392", "0.412", "0.411", "0.448", "0.467", "0.432", "0.431", "0.428", "0.401", "0.426"], ["0.383", "0.346", "0.434", "0.326", "0.371", "0.360", "0.343", "0.354", "0.388", "0.427", "0.423", "0.378"], ["0.259", "0.290", "0.273", "0.272", "0.272", "0.254", "0.273", "0.280", "0.241", "0.256", "0.264", "0.267"]], "hoverinfo": "text", "zmin": 0.0, "zmax": 1.0}], "layout": {"title": "Zero-Shot In-Context Multi-Choice Question-Answering (Belebele dataset)", "xaxis": {"title": "Language"}, "yaxis": {"title": "Model", "autorange": "reversed"}, "margin": {"l": 300}}};
        Plotly.newPlot('heatmap', heatmapChart.data, heatmapChart.layout);

    </script>
</body>
</html>
